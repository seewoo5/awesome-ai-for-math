---
title: Benchmark
layout: default
---

# Benchmark papers

| Title | Subject(s) | Venue & Year | Links & Resources |
| :--- | :--- | :--- | :--- |
| **[EternalMath: A Living Benchmark of Frontier Mathematics that Evolves with Human Discovery](https://arxiv.org/abs/2601.01400)** | [Benchmark](benchmark.md), [LLM](llm.md) | arXiv 2026 |  |
| **[FIMO: A Challenge Formal Dataset for Automated Theorem Proving](https://arxiv.org/abs/2309.04295)** | [Benchmark](benchmark.md), [ATP](atp.md) | arXiv 2023 |  |
| **[First Proof](https://arxiv.org/abs/2602.05192)** | [Benchmark](benchmark.md), [LLM](llm.md) | arXiv 2026 | [Website](https://1stproof.org/) |
| **[FormalMATH: Benchmarking Formal Mathematical Reasoning of Large Language Models](https://arxiv.org/abs/2505.02735)** | [Benchmark](benchmark.md), [ATP](atp.md), [LLM](llm.md) | arXiv 2025 | [Website](https://spherelab.ai/FormalMATH/) |
| **[FrontierMath: A Benchmark for Evaluating Advanced Mathematical Reasoning in AI](https://arxiv.org/abs/2411.04872)** | [Benchmark](benchmark.md), [LLM](llm.md) | arXiv 2024 | [Website](https://epoch.ai/frontiermath) |
| **[GAUSS: Benchmarking Structured Mathematical Skills for Large Language Models](https://arxiv.org/abs/2509.18122)** | [Benchmark](benchmark.md), [LLM](llm.md) | arXiv 2025 | [Website](https://gaussmath.ai/) |
| **[HARDMath: A Benchmark Dataset for Challenging Problems in Applied Mathematics](https://arxiv.org/abs/2410.09988)** | [Benchmark](benchmark.md), [LLM](llm.md) | arXiv 2024 | [Code](https://github.com/sarahmart/HARDMath) |
| **[Humanity's Last Exam](https://arxiv.org/abs/2501.14249)** | [Benchmark](benchmark.md), [LLM](llm.md) | arXiv 2025 | [Website](https://lastexam.ai) |
| **[IMProofBench: Benchmarking AI on Research-Level Mathematical Proof Generation](https://arxiv.org/abs/2509.26076)** | [Benchmark](benchmark.md), [LLM](llm.md) | arXiv 2025 | [Website](https://improofbench.math.ethz.ch/) |
| **[Mathematical Capabilities of ChatGPT](https://arxiv.org/abs/2301.13867)** | [Benchmark](benchmark.md), [LLM](llm.md) | NeurIPS 2023 | [Code](https://github.com/friederrr/GHOSTS) |
| **[MiniF2F: a cross-system benchmark for formal Olympiad-level mathematics](https://arxiv.org/abs/2109.00110)** | [Benchmark](benchmark.md), [ATP](atp.md) | ICLR 2022 |  |
| **[Proof or Bluff? Evaluating LLMs on 2025 USA Math Olympiad](https://arxiv.org/abs/2503.21934)** | [Benchmark](benchmark.md), [LLM](llm.md) | arXiv 2025 |  |
| **[ProofNet: Autoformalizing and Formally Proving Undergraduate-Level Mathematics](https://arxiv.org/abs/2302.12433)** | [Benchmark](benchmark.md), [ATP](atp.md) | arXiv 2023 |  |
| **[Putnam-AXIOM: A Functional and Static Benchmark for Measuring Higher Level Mathematical Reasoning in LLMs](https://arxiv.org/abs/2508.08292)** | [Benchmark](benchmark.md), [LLM](llm.md) | arXiv 2025 |  |
| **[PutnamBench: Evaluating Neural Theorem-Provers on the Putnam Mathematical Competition](https://arxiv.org/abs/2407.11214)** | [Benchmark](benchmark.md), [ATP](atp.md) | NeurIPS 2024 | [Code](https://github.com/trishullab/PutnamBench) [Website](https://trishullab.github.io/PutnamBench/) |
| **[RealMath: A Continuous Benchmark for Evaluating Language Models on Research-Level Mathematics](https://arxiv.org/abs/2505.12575)** | [Benchmark](benchmark.md), [LLM](llm.md) | NeurIPS 2025 |  |